# Paper Abstract Summaries

## ATP LLaVA Adaptive Token Pruning for Large Vision

Large Vision Language Models (LVLMs) have achieved sig- nificant success across multi-modal tasks. However, the

---

## Antidote A Unified Framework for Mitigating LVLM

Object hallucination has been an Achilles’ heel

---

## Critic V VLM Critics Help Catch VLM Errors in Mul

Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However,

---

## Discriminative Fine tuning of LVLMs

Contrastively-trained Vision-Language Models (VLMs) like

---

## DocVLM Make Your VLM an Efficient Reader

Vision-Language Models (VLMs) excel in diverse visual tasks but face challenges in document understanding, which requires fine-grained text processing. While typical vi- sual tasks perform well with low-resolution inputs, reading- intensive applications demand high-resolution, resulting in significant computational overhead. Using OCR-extracted text in VLM prompts partially addresses this issue but un- derperforms compared to full-resolution counterpart, as it lacks the complete visual context needed for optimal perfor- mance.

---

## Embodied Scene Understanding for Vision Language M

Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility ap- plications. However, a standardized, closed-loop bench- mark for evaluating their spatial reasoning and sequen- tial decision-making capabilities is lacking.

---

## FLAIR VLM with Fine grained Language informed Ima

CLIP has shown impressive results in aligning images and texts at scale. However, its ability to capture detailed vi- sual features remains limited because CLIP matches im- ages and texts at a global level. To address this is- sue, we propose FLAIR, Fine-grained Language-informed Image Representations, an approach that utilizes long

---

## FastVLM Efficient Vision Encoding for Vision Lang

Scaling the input image resolution is essential for enhancing the performance of Vision Language Mod- els (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs be-

---

## Forensics Bench A Comprehensive Forgery Detection

Compositional reasoning capabilities are usually

---

## GFlowVLM Enhancing Multi step Reasoning in Vision

Vision-Language Models (VLMs) have recently shown promising advancements in sequential decision-making tasks through task-specific fine-tuning. However, common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), present notable limitations: SFT assumes Independent and Identically Distributed (IID) data, while PPO focuses on maximizing cumulative re- wards.

---

## Global Local Tree Search in VLMs for 3D Indoor Sce

In this paper, we propose a novel model called SGFormer, Semantic Graph TransFormer for point cloud-based 3D scene graph generation. The task aims to parse a point cloud-based scene into a semantic structural graph, with the core chal- lenge of modeling the complex global structure. In con- trast, SGFormer uses Transformer layers as the base build- ing block to allow global information passing, with two types of newly-designed layers tailored for the 3D scene graph generation task.

---

## Ground V Teaching VLMs to Ground Complex Instruct

In this setting, subsequent data streams system- atically introduce novel classes that are disjoint from those seen in pre- vious training phases, while also remaining distinct from the unseen test classes. In response, we present Dynamic Prompt and Representation Learner (DPaRL), a simple yet effective Prompt-based CL (PCL) method. Our DPaRL learns to generate dynamic prompts for inference, as op- posed to relying on a static prompt pool in previous PCL methods.

---

## ImagineFSL Self Supervised Pretraining Matters on

Few-shot classiﬁcation is a challenging problem as only very few training examples are given for each new task. One of the effective research lines to address this challenge fo- cuses on learning deep representations driven by a similar-

---

## LayoutVLM Differentiable Optimization of 3D Layou

Spatial reasoning is a fundamental aspect of human cogni- tion, enabling intuitive understanding and manipulation of objects in three-dimensional space. While foundation models demonstrate remarkable performance on some benchmarks, they still struggle with 3D reasoning tasks like arranging objects in space according to open-ended language in- structions, particularly in dense and physically constrained environments. We introduce LAYOUTVLM, a framework

---

## Lifelong Knowledge Editing for Vision Language Mod

Model editing aims to correct inaccurate knowledge, up- date outdated information, and incorporate new data into Large Language Models (LLMs) without the need for re- training. While some editors demonstrate strong ro- bustness for lifelong editing in pure LLMs, Vision LLMs (VLLMs), which incorporate an additional vision modal- ity, are not directly adaptable to existing LLM editors. In this paper, we propose LiveEdit, a Lifelong vision language model Edit to bridge the gap between lifelong LLM edit- ing and VLLMs.

---

## MASH VLM Mitigating Action Scene Hallucination in

In this work, we tackle the challenging problem of unsu- pervised video domain adaptation (UVDA) for action recog- nition. We specifically focus on scenarios with a substantial domain gap, in contrast to existing works primarily deal

---

## MIMO A medical vision language model with visual

While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce ARIA, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. We pre-train ARIA from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal under- standing, long context window, and instruction following.

---

## Overcoming Shortcut Problem in VLM for Robust Out

Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have

---

## PARC A Quantitative Framework Uncovering the Symm

In recent years, the data collected for artificial intel- ligence has grown to an unmanageable amount. Partic- ularly within industrial applications, such as autonomous vehicles, model training computation budgets are being ex- ceeded while model performance is saturating – and yet more data continues to pour in. To navigate the flood of data, we propose a framework to select the most semanti- cally diverse and important dataset portion.

---

## SPA VL A Comprehensive Safety Preference Alignmen

The emergence of Vision Language Models (VLMs) has brought unprecedented advances in understanding multi- modal information. The combination of textual and vi- sual semantics in VLMs is highly complex and diverse, making the safety alignment of these models challenging. In terms of breadth, SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcate- gories, and contains 100,788 samples of the quadruple (question, image, chosen response, rejected response).

---

## Stealthy Backdoor Attack in Self Supervised Learni

Self-supervised learning (SSL) vision encoders learn high- quality image representations and thus have become a vi- tal part of developing vision modality of large vision lan- guage models (LVLMs). Due to the high cost of train- ing such encoders, pre-trained encoders are widely shared and deployed into many LVLMs, which are security-critical or bear societal significance. Under this practical sce- nario, we reveal a new backdoor threat that significant

---

## Steering Away from Harm An Adaptive Approach to D

Vision Language Models (VLMs) can produce unintended and harmful content when exposed to adversarial attacks,

---

## TopV Compatible Token Pruning with Inference Time

The emergence of Mixture of Experts (MoE) LLMs has significantly advanced the develop- ment of language models. Compared to tra- ditional LLMs, MoE LLMs outperform tradi-

---

## Towards Vision Language Models For Extra Long Vide

Long video understanding poses a significant challenge for current Multi-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained by their limited con-

---

## VLsI Verbalized Layers to Interactions from Large

The recent surge in high-quality visual instruction tun- ing samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger mod- els brings significant computational challenges, especially for deployment on resource-constrained devices like mo- bile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes effi- ciency without compromising accuracy.

---

## VisionZip Longer is Better but Not Necessary in V

Recent advancements in vision-language models have en- hanced performance by increasing the length of visual to- kens, making them much longer than text tokens and signif- icantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effec-

---

## What s in the Image A Deep Dive into the Vision o

Vision-Language Models (VLMs) have recently demon- strated remarkable capabilities in comprehending com- plex visual content. We reveal several key insights about how these models pro- cess visual data: (i) the internal representation of the query tokens (e.g., representations of ”describe the image”), is utilized by VLMs to store global image information; we demonstrate that these models generate surprisingly de- scriptive responses solely from these tokens, without direct access to image tokens. (ii) Cross-modal information flow is predominantly influenced by the middle layers (approxi- mately 25% of all layers), while early and late layers con- tribute only marginally.

---


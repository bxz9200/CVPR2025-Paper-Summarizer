<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Generated Paper Summaries</title>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --light-bg: #f9f9f9;
            --medium-bg: #e8f4f8;
            --border-color: #eee;
            --text-color: #333;
            --light-text: #7f8c8d;
            --method-bg: #ebf5eb;
            --method-border: #28a745;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            max-width: 1000px;
            margin: 0 auto;
            padding: 30px;
            color: var(--text-color);
            background-color: #fff;
        }
        
        h1 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 15px;
            margin-bottom: 30px;
            font-size: 2.2em;
            text-align: center;
            font-weight: bold;
        }
        
        h2 {
            color: var(--secondary-color);
            margin-top: 40px;
            font-size: 1.8em;
            border-left: 4px solid var(--secondary-color);
            padding-left: 15px;
            font-weight: bold;
        }
        
        h3 {
            color: var(--primary-color);
            font-size: 1.3em;
            margin-top: 25px;
            border-bottom: 1px dotted var(--border-color);
            padding-bottom: 5px;
            font-weight: bold;
        }
        
        .paper-container {
            margin-bottom: 60px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 25px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            transition: box-shadow 0.3s ease;
        }
        
        .paper-container:hover {
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .abstract {
            background-color: var(--light-bg);
            padding: 20px;
            border-radius: 8px;
            font-style: italic;
            text-align: justify;
            margin-bottom: 20px;
            line-height: 1.7;
            border-left: 3px solid var(--secondary-color);
            font-weight: normal;
        }
        
        .summary {
            background-color: var(--medium-bg);
            padding: 20px;
            border-radius: 8px;
            text-align: justify;
            line-height: 1.8;
            font-size: 1.05em;
            border-left: 3px solid var(--accent-color);
            font-weight: normal;
        }
        
        .method-description {
            background-color: var(--method-bg);
            padding: 20px;
            border-radius: 8px;
            text-align: justify;
            line-height: 1.8;
            font-size: 1.05em;
            border-left: 3px solid var(--method-border);
            margin-bottom: 20px;
            font-weight: normal;
        }
        
        .method-description strong {
            color: var(--method-border);
            font-weight: bold;
        }
        
        .keywords {
            background-color: #f0f7fa;
            padding: 12px;
            border-radius: 8px;
            margin-bottom: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            font-weight: normal;
        }
        
        .keywords span {
            display: inline-block;
            background-color: #e1e8ed;
            padding: 5px 10px;
            border-radius: 20px;
            font-size: 0.9em;
            color: var(--primary-color);
            font-weight: normal;
            transition: background-color 0.2s;
        }
        
        .keywords span:hover {
            background-color: var(--secondary-color);
            color: white;
        }
        
        .generation-info {
            color: var(--light-text);
            font-size: 0.9em;
            margin-bottom: 40px;
            text-align: center;
            font-weight: normal;
        }
        
        .section-title {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
            font-weight: bold;
        }
        
        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background-color: var(--secondary-color);
            margin-right: 8px;
            display: inline-block;
            border-radius: 50%;
        }
        
        .method-section-title::before {
            background-color: var(--method-border);
        }
        
        details {
            margin-top: 15px;
            background-color: #f5f5f5;
            border-radius: 5px;
            padding: 5px 15px;
        }
        
        summary {
            cursor: pointer;
            font-weight: bold;
            padding: 8px 0;
        }
        
        summary:hover {
            color: var(--secondary-color);
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            .paper-container {
                padding: 15px;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.5em;
            }
        }
        
        /* Fix for ML acronyms to prevent them from being broken */
        .no-break {
            white-space: nowrap;
        }
    </style>
</head>
<body>
    <h1>AI-Generated Paper Summaries</h1>
    <p class="generation-info">Generated on: 2025-03-20 11:58:17</p>
<div class="paper-container">
    <h2>Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Model 2502.18290v2</h2>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>encoders</span>
        <span>lvlms</span>
        <span>vision</span>
        <span>backdoor</span>
        <span>badvision</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose BADVISION, the first method to exploit this vulnerability in SSL vision encoders for LVLMs with novel trigger optimization and backdoor learning techniques.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Self-supervised learning (SSL) vision encoders learn high quality image representations and thus have become a vital part of developing vision modality of large vision language models (LVLMs). Due to the high cost of training such encoders, pre-trained encoders are widely shared and deployed into many LVLMs, which are security-critical or bear societal significance. Under this practical scenario, we reveal a new backdoor threat that significant visual hallucinations can be induced into these LVLMs by merely compromising vision encoders. Because of the sharing and reuse of these encoders, many downstream LVLMs may inherit backdoor behaviors from encoders, leading to widespread backdoors. In this work, we propose BADVISION, the first method to exploit this vulnerability in SSL vision encoders for LVLMs with novel trigger optimization and backdoor learning techniques. We evaluate BADVISION on two types of SSL encoders and LVLMs across eight benchmarks. We show that BADVISION effectively drives the LVLMs to attacker-chosen hallucination with over 99% attack success rate, causing a 77.6% relative visual understanding error while maintaining the stealthiness. So TA backdoor detection methods cannot detect our attack effectively.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Self-supervised learning (SSL) vision encoders learn high quality image representations. We show that BADVISION effectively drives the LVLMs to attacker-chosen hallucination with over 99% attack success rate. The attack causes a 77. 6% relative visual understanding error while maintaining the stealthiness. So TA backdoor detection methods cannot detect our attack effectively. We evaluate BAD VISION on two types of SSL encoder and LVLm across eight benchmarks.</div>
</div>
<div class="paper-container">
    <h2>VLs I Verbalized Layers-to-Interactions from Large to Small Vision Language Models 2412.01822v1</h2>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>vlms</span>
        <span>model</span>
        <span>vision-language</span>
        <span>models</span>
        <span>gpt-4v</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose VLs I :Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLs I :Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLs I leverages a unique, layer-wise distillation process, introducing intermediate “verbalizers” that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. Thisapproach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs’ layer-wise progression with that of the large ones. We validate VLs I across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. VLs I leverages a unique, layer-wise distillation process, introducing intermediate “verbalizers” that map features from each layer to natural language space. Thisapproach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning.</div>
</div>
<div class="paper-container">
    <h2>GFlow VLM Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks 2503.06514v1</h2>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>tasks</span>
        <span>fine-tuning</span>
        <span>gflow</span>
        <span>vlms</span>
        <span>models</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce a novel framework, GFlow VLM, a framework that fine-tune VLMs using Generative Flow Networks (GFlow Nets) to promote generation of diverse solutions for complex reasoning tasks.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Vision-Language Models (VLMs) have recently shown promising advancements in sequential decision-making tasks through task-specific fine-tuning. However, common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), present notable limitations: SFT assumes Independent and Identically Distributed (IID) data, while PPO focuses on maximizing cumulative rewards. These limitations often restrict solution diversity and hinder generalization in multi-step reasoning tasks. To address these challenges, we introduce a novel framework, GFlow VLM, a framework that fine-tune VLMs using Generative Flow Networks (GFlow Nets) to promote generation of diverse solutions for complex reasoning tasks. GFlow VLM models the environment as a non-Markovian decision process, allowing it to capture long-term dependencies essential for real-world applications. It takes observations and task descriptions as inputs to prompt chain-of-thought (Co T) reasoning which subsequently guides action selection. We use task based rewards to fine-tune VLM with GFlow Nets. This approach enables VLMs to outperform prior fine-tuning methods, including SFT and RL. Empirical results demonstrate the effectiveness of GFlow VLM on complex tasks such as card games (Number Line, Black Jack) and embodied planning tasks (ALFWorld), showing enhanced training efficiency, solution diversity, and stronger generalization capabilities across both in-distribution and out-of-distribution scenarios.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">GFlow VLM Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks. GFlow V LM models the environment as a non-Markovian decision process. It takes observations and task descriptions as inputs to prompt chain-of-thought (Co T) reasoning which guides action selection. We use task based rewards to fine-tune VLM with GFlow Nets. This approach enables VLMs to outperform prior fine- tuning methods, including SFT and RL.</div>
</div>
<div class="paper-container">
    <h2>Forensics-Bench A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models 2404.16006v1</h2>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>multimodal</span>
        <span>mmt-bench</span>
        <span>comprehensive</span>
        <span>benchmark</span>
        <span>large</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI Kaining Ying 1Fanqing Meng 2 1Jin Wang 3Zhiqian Li1 3Han Lin2 1Yue Yang2 1Hao Zhang1 Wenbo Zhang4Yuqi Lin1 5Shuo Liu1Jiayi Lei1 2Quanfeng Lu1Runjian Chen1 3Peng Xu1 3 Renrui Zhang1Haozhe Zhang5Peng Gao1Yali Wang6Yu Qiao1Ping Luo3 1Kaipeng Zhang†1 Wenqi Shao†1 Abstract Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises 325meticulously curated multi-choi</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodals tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. The study comprises 325meticulously curated multi-choi tasks. The results of the study were published in the journal Forensics-Bench 2404-16006.</div>
</div>
<div class="paper-container">
    <h2>Embodied Scene Understanding for Vision Language Models via Meta VQA 2501.09167v1</h2>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>vlms</span>
        <span>spatial</span>
        <span>embodied</span>
        <span>closed-loop</span>
        <span>benchmark</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we present Meta VQA: a comprehensive benchmark designed to assess and enhance VLMs’ understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present Meta VQA: a comprehensive benchmark designed to assess and enhance VLMs’ understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations. Meta VQA leverages Set-of-Mark prompting and top-down view ground-truth annotations from nu Scenes and Waymo datasets to automatically generate extensive question-answer pairs based on diverse real-world traffic scenarios, ensuring object-centric and context-rich instructions. Our experiments show that fine-tuning VLMs with the Meta VQA dataset significantly improves their spatial reasoning and embodied scene comprehension in safetycritical simulations, evident not only in improved VQA accuracies but also in emerging safety-aware driving maneuvers. In addition, the learning demonstrates strong transferability from simulation to real-world observation. Code and data will be publicly available at https:// metadriverse.github.io/metavqa .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present Meta VQA: a comprehensive benchmark designed to assess and enhance VLMs’ understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed- loop simulations. Our experiments show that fine-tuning VL Ms with the Meta V QA dataset significantly improves their spatial. reasoning and embodied scene comprehension in safetycritical simulations.</div>
</div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Generated Paper Summaries</title>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --light-bg: #f9f9f9;
            --medium-bg: #e8f4f8;
            --border-color: #eee;
            --text-color: #333;
            --light-text: #7f8c8d;
            --method-bg: #ebf5eb;
            --method-border: #28a745;
            --layout-badge: #9b59b6;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            max-width: 1000px;
            margin: 0 auto;
            padding: 30px;
            color: var(--text-color);
            background-color: #fff;
        }
        
        h1 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 15px;
            margin-bottom: 30px;
            font-size: 2.2em;
            text-align: center;
            font-weight: bold;
        }
        
        h2 {
            color: var(--secondary-color);
            margin-top: 40px;
            font-size: 1.8em;
            border-left: 4px solid var(--secondary-color);
            padding-left: 15px;
            font-weight: bold;
        }
        
        h3 {
            color: var(--primary-color);
            font-size: 1.3em;
            margin-top: 25px;
            border-bottom: 1px dotted var(--border-color);
            padding-bottom: 5px;
            font-weight: bold;
        }
        
        .paper-container {
            margin-bottom: 60px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 25px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            transition: box-shadow 0.3s ease;
        }
        
        .paper-container:hover {
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            margin-bottom: 15px;
        }
        
        .paper-title {
            flex: 1;
            margin: 0;
        }
        
        .layout-badge {
            display: inline-block;
            background-color: var(--layout-badge);
            color: white;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.8em;
            margin-left: 10px;
        }
        
        .abstract {
            background-color: var(--light-bg);
            padding: 20px;
            border-radius: 8px;
            font-style: italic;
            text-align: justify;
            margin-bottom: 20px;
            line-height: 1.7;
            border-left: 3px solid var(--secondary-color);
            font-weight: normal;
        }
        
        .summary {
            background-color: var(--medium-bg);
            padding: 20px;
            border-radius: 8px;
            text-align: justify;
            line-height: 1.8;
            font-size: 1.05em;
            border-left: 3px solid var(--accent-color);
            font-weight: normal;
        }
        
        .method-description {
            background-color: var(--method-bg);
            padding: 20px;
            border-radius: 8px;
            text-align: justify;
            line-height: 1.8;
            font-size: 1.05em;
            border-left: 3px solid var(--method-border);
            margin-bottom: 20px;
            font-weight: normal;
        }
        
        .method-description strong {
            color: var(--method-border);
            font-weight: bold;
        }
        
        .keywords {
            background-color: #f0f7fa;
            padding: 12px;
            border-radius: 8px;
            margin-bottom: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            font-weight: normal;
        }
        
        .keywords span {
            display: inline-block;
            background-color: #e1e8ed;
            padding: 5px 10px;
            border-radius: 20px;
            font-size: 0.9em;
            color: var(--primary-color);
            font-weight: normal;
            transition: background-color 0.2s;
        }
        
        .keywords span:hover {
            background-color: var(--secondary-color);
            color: white;
        }
        
        .generation-info {
            color: var(--light-text);
            font-size: 0.9em;
            margin-bottom: 40px;
            text-align: center;
            font-weight: normal;
        }
        
        .section-title {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
            font-weight: bold;
        }
        
        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background-color: var(--secondary-color);
            margin-right: 8px;
            display: inline-block;
            border-radius: 50%;
        }
        
        .method-section-title::before {
            background-color: var(--method-border);
        }
        
        .paper-metadata {
            background-color: #f8f9fa;
            border-radius: 6px;
            padding: 10px 15px;
            margin-top: 20px;
            font-size: 0.9em;
            color: #666;
        }
        
        .paper-metadata ul {
            list-style-type: none;
            padding: 0;
            margin: 5px 0;
        }
        
        .paper-metadata li {
            margin: 3px 0;
        }
        
        details {
            margin-top: 15px;
            background-color: #f5f5f5;
            border-radius: 5px;
            padding: 5px 15px;
        }
        
        summary {
            cursor: pointer;
            font-weight: bold;
            padding: 8px 0;
        }
        
        summary:hover {
            color: var(--secondary-color);
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            .paper-container {
                padding: 15px;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.5em;
            }
        }
        
        /* Fix for ML acronyms to prevent them from being broken */
        .no-break {
            white-space: nowrap;
        }
    </style>
</head>
<body>
    <h1>AI-Generated Paper Summaries</h1>
    <p class="generation-info">Generated on: 2025-03-21 12:20:42</p>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">MAC-Ego3D Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3 2412.09723v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>gaussian</span>
        <span>mac-ego3d</span>
        <span>multi-agent</span>
        <span>maps</span>
        <span>consensus</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we present MAC-Ego3D, a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Real-time multi-agent collaboration for ego-motion estimation and high-fidelity 3D reconstruction is vital for scalable spatial intelligence. However, traditional methods produce sparse, low-detail maps, while recent dense mapping approaches struggle with high latency. To overcome these challenges, we present MAC-Ego3D, a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D enables agents to independently construct, align, and iteratively refine local maps using a unified Gaussian splat representation. Through Intra-Agent Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian splats within an agent. For global alignment, parallelized Inter-Agent Gaussian Consensus, which asynchronously aligns and optimizes local maps by regularizing multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D rendering, enabling rapid inter-agent Gaussian association and alignment. MAC-Ego3D bridges local precision and global coherence, delivering higher efficiency, largely reducing localization error, and improving mapping fidelity. It establishes a new SOTA on synthetic and real-world benchmarks, achieving a 15×increase in inference speed,order-of-magnitude reductions in ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10 d B . Our code will be made publicly available at [URL] .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">MAC-Ego3D is a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. It enables agents to independently construct, align, and iteratively refine local maps using a unified Gaussian splat representation. It establishes a new SOTA on synthetic and real-world benchmarks, achieving a 15×increase in inference speed, order-of-magnitude reductions in ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10 d B.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">From Multimodal LLMs to Generalist Embodied Agents Methods and Lessons 2412.08442v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>models</span>
        <span>generalist</span>
        <span>model</span>
        <span>large</span>
        <span>language</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">We examine the capability of Multimodal Large Language Models (MLLMs) to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. Specifically, our focus lies in areas such as Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. GEA is trained with supervised learning on a large dataset of embodied experiences and with online RL in interactive simulators. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents. The final GEA model achieves strong generalization performance to unseen tasks across diverse benchmarks compared to other generalist models and benchmark-specific approaches.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">We examine the capability of Multimodal Large Language Models to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA) GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Unlocking Video-LLM via Agent-of-Thoughts Distillation 2412.01694v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>models</span>
        <span>paper</span>
        <span>video</span>
        <span>reasoning</span>
        <span>large</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose Agentof-Thoughts Distillation ( Ao TD ), a method that enhances models by incorporating automatically generated Chainof-Thoughts (Co Ts) into the instruction-tuning process.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">This paper tackles the problem of video question answering (Video QA), a task that often requires multi-step reasoning and a profound understanding of spatial-temporal dynamics. While large video-language models perform well on benchmarks, they often lack explainability and spatialtemporal grounding. In this paper, we propose Agentof-Thoughts Distillation ( Ao TD ), a method that enhances models by incorporating automatically generated Chainof-Thoughts (Co Ts) into the instruction-tuning process. Specifically, we leverage an agent-based system to decompose complex questions into sub-tasks, and address them with specialized vision models, the intermediate results are then treated as reasoning chains. We also introduce a verification mechanism using a large language model (LLM) to ensure the reliability of generated Co Ts. Extensive experiments demonstrate that Ao TD improves the performance on multiple-choice and open-ended benchmarks.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Tackles the problem of video question answering (Video QA), a task that often requires multi-step reasoning and a profound understanding of spatial-temporal dynamics. While large video-language models perform well on benchmarks, they often lack explainability and spatialtemporal grounding. In this paper, we propose Agentof-Thoughts Distillation ( Ao TD ), a method that enhances models by incorporating automatically generated Chain of Thoughts (Co Ts) into the instruction-tuning process. We also introduce a verification mechanism using a large language model (LLM) to ensure the reliability of generated Co Ts.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Learning to Detect Objects from Multi-Agent Li DAR Scans without Manual Labels 2503.08421v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>labels</span>
        <span>object</span>
        <span>unsupervised</span>
        <span>detection</span>
        <span>collaborative</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce a novel unsupervised method that learns to Detect Objects from Mul ti-Agent Li DAR scans, termed DOt A, without using labels from external.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Unsupervised 3D object detection serves as an important solution for offline 3D object annotation. However, due to the data sparsity and limited views, the clusteringbased label fitting in unsupervised object detection often generates low-quality pseudo-labels. Multi-agent collaborative dataset, which involves the sharing of complementary observations among agents, holds the potential to break through this bottleneck. In this paper, we introduce a novel unsupervised method that learns to Detect Objects from Mul ti-Agent Li DAR scans, termed DOt A, without using labels from external. DOt A first uses the internally shared ego-pose and ego-shape of collaborative agents to initialize the detector, leveraging the generalization performance of neural networks to infer preliminary labels. Subsequently, DOt A uses the complementary observations between agents to perform multi-scale encoding on preliminary labels, then decodes high-quality and low-quality labels. These labels are further used as prompts to guide a correct feature learning process, thereby enhancing the performance of the unsupervised object detection task. Extensive experiments on the V2V4Real and OPV2V datasets show that our DOt A outperforms state-of-the-art unsupervised 3D object detection methods. Additionally, we also validate the effectiveness of the DOt A labels under various collaborative perception frameworks. The code is available at[URL] A .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Unsupervised object detection often generates low-quality pseudo-labels. As a solution, As a solution, As a solution, We introduce a novel unsupervised method that learns to Detect Objects from Mul ti-Agent Li DAR scans, termed DOt A, throughout utilizing labels from external. Extensive experiments on the V2V4Real and OPV2V datasets show that our method outperforms state-of-the-art 3D object detection methods. The code is available at[URL] A. As a solution, As a solution, As a solution, In this paper, we introduce a new method to learn to detect 3D objects throughout the use of manual labels.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">NADER Neural Architecture Design via Multi-Agent Collaboration 2412.19206v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>neural</span>
        <span>architectures</span>
        <span>architecture</span>
        <span>search</span>
        <span>methods</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose the Reflector, which effectively learns from immediate feedback and long-term experiences.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Designing effective neural architectures poses a significant challenge in deep learning. While Neural Architecture Search (NAS) automates the search for optimal architectures, existing methods are often constrained by predetermined search spaces and may miss critical neural architectures. In this paper, we introduce NADER (Neural Architecture Design via multi-ag Ent collabo Ration), a novel framework that formulates neural architecture design (NAD) as a LLM-based multi-agent collaboration problem. NADER employs a team of specialized agents to enhance a base architecture through iterative modification. Current LLMbased NAD methods typically operate independently, lacking the ability to learn from past experiences, which results in repeated mistakes and inefficient exploration. To address this issue, we propose the Reflector, which effectively learns from immediate feedback and long-term experiences. Additionally, unlike previous LLM-based methods that use code to represent neural architectures, we utilize a graph-based representation. This approach allows agents to focus on design aspects without being distracted by coding. We demonstrate the effectiveness of NADER in discovering high-performing architectures beyond predetermined search spaces through extensive experiments on benchmark tasks, showcasing its advantages over state-of-the-art methods. The codes will be released soon.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">NADER (Neural Architecture Design via multi-ag Ent collabo Ration) is a novel framework that formulates neural architecture design as a LLM-based multi-agent collaboration problem. Current LLMbased NAD methods typically operate independently, lacking the ability to learn from past experiences, which results in repeated mistakes and inefficient exploration. To address this issue, we propose the Reflector, which effectively learns from immediate feedback and long-term experiences. This approach allows agents to focus on design aspects throughout being distracted by coding.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Optimus-2 Mulitimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy 2502.19902v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>tasks</span>
        <span>behavior</span>
        <span>minecraft</span>
        <span>actions</span>
        <span>language</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose Optimus-a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for highlevel planning, alongside a Goal-Observation Action Conditioned Policy (GOAP) for low-level control.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a longterm goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for highlevel planning, alongside a Goal-Observation Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixedlength behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation Action (MGOA) dataset, which contains 000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community’s efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft. Please see the</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Optimus-2 is a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for highlevel planning, alongside a Goal-Observation Action Conditioned Policy (GOAP) for low-level control. GOAP contains an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Show UI One Vision-Language-Action Model for GUI Visual Agent 2411.17465v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>visual</span>
        <span>agents</span>
        <span>model</span>
        <span>token</span>
        <span>selection</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we develop a visionlanguage-action model in digital world, namely Show UI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a visionlanguage-action model in digital world, namely Show UI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, Show UI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4×. Navigation experiments across web, mobile, and online environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at [URL] UI .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">The academic paper abstract is broken down into a coherent, logical paragraph. It includes key technical details and significant results. For more information on the models, visit the UI website or go to: http: //www. ibm. com/uibm/ui/ui-vision-language-action-model. The models are available at [URL] UI. Back to Mail Online home. back to the page you came from. The article was published on November 14, 2013.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">RL-RC-Do T A Block-level RL agent for Task-Aware Video Compression 2501.12216v1</h2>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>task</span>
        <span>downstream</span>
        <span>video</span>
        <span>encoders</span>
        <span>compression</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">Our approach improves task performance for a given bit rate compared to traditional taskagnostic encoding methods, paving the way for more efficient task-aware video compression.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional taskagnostic encoding methods, paving the way for more efficient task-aware video compression.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. We address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. Our approach improves task performance for a given bit rate compared to traditional taskagnostic encoding methods, paving the way for more efficient task-aware video compression. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding.</div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Spirit Sight Agent Advanced GUI Agent with One Look 2503.03196v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>spirit</span>
        <span>navigation</span>
        <span>agent</span>
        <span>compatibility</span>
        <span>user</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose Spirit Sight, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various GUI platforms.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Graphical User Interface (GUI) agents show amazing abilities in assisting human-computer interaction, automating human user’s navigation on digital devices. An ideal GUI agent is expected to achieve high accuracy, low latency, and compatibility for different GUI platforms. Recent visionbased approaches have shown promise by leveraging advanced Vision Language Models (VLMs). While they generally meet the requirements of compatibility and low latency, these vision-based GUI agents tend to have low accuracy due to their limitations in element grounding. To address this issue, we propose Spirit Sight, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various GUI platforms. First, we create a multilevel, large-scale, high-quality GUI dataset called GUILasagne using scalable methods, empowering Spirit Sight with robust GUI understanding and grounding capabilities. Second, we introduce the Universal Block Parsing (UBP) method to resolve the ambiguity problem in dynamic high-resolution of visual inputs, further enhancing Spirit Sight’s ability to ground GUI objects. Through these efforts, Spirit Sight agent outperforms other advanced methods on diverse GUI benchmarks, demonstrating its superior capability and compatibility in GUI navigation tasks. Models are available at [URL] Sense LLM/Spirit Sight-Agent-8B .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Graphical User Interface (GUI) agents show amazing abilities in assisting human-computer interaction, automating human user’s navigation on digital devices. An ideal GUI agent is expected to achieve high accuracy, low latency, and compatibility for different GUI platforms. To address this issue, we propose Spirit Sight, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various platforms. We create a multilevel, large-scale, high-quality GUI dataset called GUILasagne using scalable methods, empowering Spirit Sight with robust GUI understanding and grounding capabilities.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">TANGO Training-free Embodied AI Agents for Open-world Tasks 2412.10402v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>tasks</span>
        <span>navigation</span>
        <span>llms</span>
        <span>capabilities</span>
        <span>images</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple Point Goal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set Object Goal Navigation, Multi Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">TANGO is an approach that extends the program composition via LLMs already observed for images. We use a simple Point Goal Navigation model combined through a memory-based exploration policy as a foundational primitive for guiding an agent through the world. We show how a single model can address diverse tasks throughout additional training. We evaluate our approach on three key Embodied AI tasks: Open-Set Object Goal Navigation, Multi Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results throughout any specific fine-tuning.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Reasoning in visual navigation of end-to-end trained agents a dynamical systems approach 2503.08306v3</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>agent</span>
        <span>behavior</span>
        <span>memory</span>
        <span>planning</span>
        <span>capabilities</span>
    </div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Progress in Embodied AI has made it possible for end-toend-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or languageconditioned behavior, but benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a largescale experimental study involving 262 navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available [here].</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Was published in the journal Embodied AI. An interactive tool is available to help students with reading comprehension and vocabulary. The abstract is as follows: Summarize the academic paper abstract into a coherent, logical paragraph. .</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Taming Large Multimodal Agents for Ultra-low Bitrate Semantically Disentangled Image Compression 2503.00399v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>semantic</span>
        <span>image</span>
        <span>reference</span>
        <span>ultra-low</span>
        <span>consistency</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">We propose a novel image compression framework, Semantically Disentangled Image Compression (SEDIC) in this paper.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">It remains a significant challenge to compress images at ultra-low bitrate while achieving both semantic consistency and high perceptual quality. We propose a novel image compression framework, Semantically Disentangled Image Compression (SEDIC) in this paper. Our proposed SEDIC leverages large multimodal models (LMMs) to disentangle the image into several essential semantic information, including an extremely compressed reference image, overall and object-level text descriptions, and the semantic masks. A multi-stage semantic decoder is designed to progressively restore the transmitted reference image object-by-object, ultimately producing high-quality and perceptually consistent reconstructions. In each decoding stage, a pre-trained controllable diffusion model is utilized to restore the object details on the reference image conditioned by the text descriptions and semantic masks. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at ultra-low bitrates ( ≤0.05 bpp). Our code is available at [URL] .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">It remains a significant challenge to compress images at ultra-low bitrate while achieving both semantic consistency and high perceptual quality. We propose a novel image compression framework, Semantically Disentangled Image Compression (SEDIC) SEDIC leverages large multimodal models to disentangle the image into several essential semantic information, including an extremely compressed reference image, overall and object-level text descriptions, and the semantic masks. A multi-stage semantic decoder is designed to progressively restore the transmitted reference image object-by-object.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-metadata">
    <h3>Summary Statistics</h3>
    <ul>
        <li>Total papers processed: 12</li>
        <li>Papers with abstracts: 12</li>
        <li>Papers with summaries: 12</li>
        <li>Papers with figures before abstract: 11</li>
    </ul>
    <details>
        <summary>Papers with Non-Standard Layouts</summary>
        <ul>
            <li>MAC-Ego3D Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3 2412.09723v1</li>
            <li>From Multimodal LLMs to Generalist Embodied Agents Methods and Lessons 2412.08442v1</li>
            <li>Unlocking Video-LLM via Agent-of-Thoughts Distillation 2412.01694v2</li>
            <li>Learning to Detect Objects from Multi-Agent Li DAR Scans without Manual Labels 2503.08421v2</li>
            <li>NADER Neural Architecture Design via Multi-Agent Collaboration 2412.19206v1</li>
            <li>Optimus-2 Mulitimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy 2502.19902v2</li>
            <li>Show UI One Vision-Language-Action Model for GUI Visual Agent 2411.17465v1</li>
            <li>Spirit Sight Agent Advanced GUI Agent with One Look 2503.03196v1</li>
            <li>TANGO Training-free Embodied AI Agents for Open-world Tasks 2412.10402v1</li>
            <li>Reasoning in visual navigation of end-to-end trained agents a dynamical systems approach 2503.08306v3</li>
            <li>Taming Large Multimodal Agents for Ultra-low Bitrate Semantically Disentangled Image Compression 2503.00399v2</li>
        </ul>
    </details>
</div>
</body>
</html>
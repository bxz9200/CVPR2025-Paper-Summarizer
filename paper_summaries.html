<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-Generated Paper Summaries</title>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --light-bg: #f9f9f9;
            --medium-bg: #e8f4f8;
            --border-color: #eee;
            --text-color: #333;
            --light-text: #7f8c8d;
            --method-bg: #ebf5eb;
            --method-border: #28a745;
            --layout-badge: #9b59b6;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            max-width: 1000px;
            margin: 0 auto;
            padding: 30px;
            color: var(--text-color);
            background-color: #fff;
        }
        
        h1 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 15px;
            margin-bottom: 30px;
            font-size: 2.2em;
            text-align: center;
            font-weight: bold;
        }
        
        h2 {
            color: var(--secondary-color);
            margin-top: 40px;
            font-size: 1.8em;
            border-left: 4px solid var(--secondary-color);
            padding-left: 15px;
            font-weight: bold;
        }
        
        h3 {
            color: var(--primary-color);
            font-size: 1.3em;
            margin-top: 25px;
            border-bottom: 1px dotted var(--border-color);
            padding-bottom: 5px;
            font-weight: bold;
        }
        
        .paper-container {
            margin-bottom: 60px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 25px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            transition: box-shadow 0.3s ease;
        }
        
        .paper-container:hover {
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            margin-bottom: 15px;
        }
        
        .paper-title {
            flex: 1;
            margin: 0;
        }
        
        .layout-badge {
            display: inline-block;
            background-color: var(--layout-badge);
            color: white;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.8em;
            margin-left: 10px;
        }
        
        .abstract {
            background-color: var(--light-bg);
            padding: 20px;
            border-radius: 8px;
            font-style: italic;
            text-align: justify;
            margin-bottom: 20px;
            line-height: 1.7;
            border-left: 3px solid var(--secondary-color);
            font-weight: normal;
        }
        
        .summary {
            background-color: var(--medium-bg);
            padding: 20px;
            border-radius: 8px;
            text-align: justify;
            line-height: 1.8;
            font-size: 1.05em;
            border-left: 3px solid var(--accent-color);
            font-weight: normal;
        }
        
        .method-description {
            background-color: var(--method-bg);
            padding: 20px;
            border-radius: 8px;
            text-align: justify;
            line-height: 1.8;
            font-size: 1.05em;
            border-left: 3px solid var(--method-border);
            margin-bottom: 20px;
            font-weight: normal;
        }
        
        .method-description strong {
            color: var(--method-border);
            font-weight: bold;
        }
        
        .keywords {
            background-color: #f0f7fa;
            padding: 12px;
            border-radius: 8px;
            margin-bottom: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            font-weight: normal;
        }
        
        .keywords span {
            display: inline-block;
            background-color: #e1e8ed;
            padding: 5px 10px;
            border-radius: 20px;
            font-size: 0.9em;
            color: var(--primary-color);
            font-weight: normal;
            transition: background-color 0.2s;
        }
        
        .keywords span:hover {
            background-color: var(--secondary-color);
            color: white;
        }
        
        .generation-info {
            color: var(--light-text);
            font-size: 0.9em;
            margin-bottom: 40px;
            text-align: center;
            font-weight: normal;
        }
        
        .section-title {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
            font-weight: bold;
        }
        
        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background-color: var(--secondary-color);
            margin-right: 8px;
            display: inline-block;
            border-radius: 50%;
        }
        
        .method-section-title::before {
            background-color: var(--method-border);
        }
        
        .paper-metadata {
            background-color: #f8f9fa;
            border-radius: 6px;
            padding: 10px 15px;
            margin-top: 20px;
            font-size: 0.9em;
            color: #666;
        }
        
        .paper-metadata ul {
            list-style-type: none;
            padding: 0;
            margin: 5px 0;
        }
        
        .paper-metadata li {
            margin: 3px 0;
        }
        
        details {
            margin-top: 15px;
            background-color: #f5f5f5;
            border-radius: 5px;
            padding: 5px 15px;
        }
        
        summary {
            cursor: pointer;
            font-weight: bold;
            padding: 8px 0;
        }
        
        summary:hover {
            color: var(--secondary-color);
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            .paper-container {
                padding: 15px;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.5em;
            }
        }
        
        /* Fix for ML acronyms to prevent them from being broken */
        .no-break {
            white-space: nowrap;
        }
    </style>
</head>
<body>
    <h1>AI-Generated Paper Summaries</h1>
    <p class="generation-info">Generated on: 2025-03-21 14:29:01</p>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">V2X-R Cooperative Li DAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection 2411.08402v3</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>radar</span>
        <span>fusion</span>
        <span>performance</span>
        <span>detection</span>
        <span>weather-robust</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a novel cooperative Li DAR-4D radar fusion pipeline for 3D object detection and implement it with multiple fusion strategies.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using Li DAR and camera data. However, they face performance degradation in adverse weather. Weather-robust 4D radar, with Doppler velocity and additional geometric information, offers a promising solution to this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating Li DAR, camera, and 4D radar modalities. V2XR contains 079 scenarios with 727 frames of Li DAR and 4D radar point clouds, 908 images, and 859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative Li DAR-4D radar fusion pipeline for 3D object detection and implement it with multiple fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to guide the diffusion model in denoising noisy Li DAR features. Experiments show that our Li DAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the foggy/snowy performance of the basic fusion model by up to 5.73%/6.70% and barely disrupting normal performance. The dataset and code will be publicly available at: [URL]</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">V2X-R is the first simulated V2X dataset incorporating Li DAR, camera, and 4D radar modalities. We propose a novel cooperative Li DAR-4D radar fusion pipeline for 3D object detection and implement it with multiple fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. The dataset and code will be publicly available at: http: //www. v2x-r. org/.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">UCOD-DPL Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning 2407.13157v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>camouflaged</span>
        <span>segmentation</span>
        <span>noisy</span>
        <span>pixels</span>
        <span>object</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a noise correction loss that facilitates the model‚Äôs learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Existing Camouflaged Object Detection(COD)methodsrely heavily on large-scale pixel-annotated training sets, which are both timeconsuming and labor-intensive. Although weakly supervised methods offerhigherannotationefficiency,theirperformanceisfarbehindduetothe unclear visual demarcations between foreground and background in camouflaged images. In this paper, we explore the potential of using boxes as prompts in camouflaged scenes and introduce the first weakly semisupervised COD method, aiming for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of fully labeled images. Critically, learning from such limited set inevitably generates pseudo labels with serious noisy pixels. To address this, we propose a noise correction loss that facilitates the model‚Äôs learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels. When using only 20% of fully labeled data, our method shows superior performance over the state-of-the-art methods. ¬∑Object segmentation</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">UCOD-DPL Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning 2407. When using only 20% of fully labeled data, our method demonstrates superior performance over the state-of-the-art methods. To address this, we propose a noise correction loss that facilitates the model‚Äôs learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels. We aim for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of Fully labeled images.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Samba A Unified Mamba-based Framework for General Salient Object Detection 2305.05260v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>reÔ¨Ånement</span>
        <span>module</span>
        <span>sub-modules</span>
        <span>gfrnet</span>
        <span>focal</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">We propose GFRNet based on the pioneering idea of leveraging Ai F and depth cues to guide the reÔ¨Ånement of focal stacks, which is conducive to light Ô¨Åeld SOD.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">for the two sub-modules, with enhancement in terms of spatial structure details and positional information, respectively. In all, our contributions in this paper are three-fold: We propose GFRNet based on the pioneering idea of leveraging Ai F and depth cues to guide the reÔ¨Ånement of focal stacks, which is conducive to light Ô¨Åeld SOD. We propose a new guided reÔ¨Ånement and fusion module (GRFM) to reÔ¨Åne focal stacks and aggregate different modalities. According to modality-speciÔ¨Åc properties, we design Ai F-guided and depth-guided reÔ¨Ånement strategies and sub-modules, namely Ai F-based reÔ¨Ånement module (ARM) and depth-based reÔ¨Ånement module (DRM). Extensive experiments show the superiority of our GFRNet against 12 state-of-the-art light Ô¨Åeld SOD methods, and the necessity of employing modality-speciÔ¨Åc reÔ¨Ånement modules is also validated.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">In all, our contributions in this paper are three-fold: We propose GFRNet based on the pioneering idea of leveraging Ai F and depth cues to guide the reÔ¨Ånement of focal stacks, which is conducive to light Ô¨Åeld SOD. According to modality-speciÔ¨Åc properties, we design Ai F-guided and depth-guided reÔ¨Ånement strategies and sub-modules, namely Ai F-based reÔ¨Ånement module (ARM) and depth-based reÔ¨Ånement module (DRM). Extensive experiments show the superiority of our GFRNet against 12 state-of-the-art light Ô¨Åeld SOD methods, and the necessity of employing modality-speciÔ¨Åc reÔ¨Ånement modules is also validated.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Sim LTD Simple Semi-Supervised Long-Tailed Object Detection 2412.20047v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>classes</span>
        <span>object</span>
        <span>detection</span>
        <span>long-tailed</span>
        <span>image</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">We propose a more versatile approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Recent years have witnessed tremendous advances on modern visual recognition systems. Despite such progress, many vision models still struggle with the open problem of learning from few exemplars. This paper focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. Existing approaches to long-tailed detection resort to external Image Net labels to augment the low-shot training instances. However, such dependency on a large labeled database is impractical and has limited utility in realistic scenarios. We propose a more versatile approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations. Our Sim LTD framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes. Our approach can be viewed as an improved head-to-tail model transfer paradigm without the added complexities of meta-learning or knowledge distillation, as was required in past research. By harnessing supplementary unlabeled images, without extra image labels, Sim LTD establishes new record results on the challenging LVIS v1 benchmark across both supervised and semi-supervised settings.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. We propose a more versatile approach to leverage optional unlabeled images, which are easy to collect throughout the burden of human annotations. Our Sim LTD framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Ev-3DOD Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras 2502.19630v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>detection</span>
        <span>camera</span>
        <span>object</span>
        <span>first</span>
        <span>autonomous</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce asynchronous event cameras into 3D object detection for the first time.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multimodal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., Li DAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC3DOD, which includes ground-truth 3D bounding boxesat 100 FPS, establishing the first benchmark for eventbased 3D detectors. The code and dataset are available at [URL] .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. We introduce the first event-based 3Dobject detection dataset, DSEC3DOD, which includes ground-truth 3D bounding boxesat 100 FPS. The code and dataset are available at [URL].</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">MI-DETR An Object Detection Model with Multi-time Inquiries Mechanism 2503.01463v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>object</span>
        <span>decoder</span>
        <span>architecture</span>
        <span>model</span>
        <span>cascaded</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">this paper proposes a new decoder architecture.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Based on analyzing the character of cascaded decoder architecture commonly adopted in existing DETR-like models, this paper proposes a new decoder architecture. The cascaded decoder architecture constrains object queries to update in the cascaded direction, only enabling object queries to learn relatively-limited information from image features. However, the challenges for object detection in natural scenes (e.g., extremely-small, heavily-occluded, and confusingly mixed with the background) require an object detection model to fully utilize image features, which motivates us to propose a new decoder architecture with the parallel Multi-time Inquiries (MI) mechanism. MIenables object queries to learn more comprehensive information, and our MIbased model, MI-DETR, outperforms all existing DETR-like models on COCO benchmark under different backbones and training epochs, achieving +2.3 AP and+0.6 AP improvements compared to the most representative model DINO and SOTA model Relation-DETR under Res Net-50 backbone. In addition, a series of diagnostic and visualization experiments demonstrate the effectiveness, rationality, and interpretability of MI.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">The MIbased model, MI-DETR, outperforms all existing DETR-like models on COCO benchmark under different backbones and training epochs. MIenables object queries to learn more comprehensive information, and a series of diagnostic and visualization experiments demonstrate the effectiveness, rationality, and interpretability of MI. The paper is based on analyzing the character of cascaded decoder architecture commonly adopted in existing DETr-like model. It is written in the form of an academic paper abstract.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Re Diff Det Rotation-equivariant Diffusion Model for Oriented Object Detection 2103.07733v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>orientation</span>
        <span>object</span>
        <span>aerial</span>
        <span>images</span>
        <span>objects</span>
    </div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Recently, object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. Therefore, the detector requires more parameters to encode the orientation information, which are often highly redundant and inefÔ¨Åcient. Moreover, as ordinary CNNs do not explicitly model the orientation variation, large amounts of rotation augmented data is needed to train an accurate object detector.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. The detector requires more parameters to encode the orientation information, which are often highly redundant and inefÔ¨Åcient. Large amounts of rotation augmented data is needed to train an accurate object detector. As a solution, As a solution, As a solution, The proposed method/approach and how it works is described in the abstract below. The abstract is broken down into four sections: the first section is the abstract, the second is the main section, and the third is the conclusion.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Point2RBox-v2 Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instan 2502.04268v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>loss</span>
        <span>layout</span>
        <span>instances</span>
        <span>point2rbox-v2</span>
        <span>gaussian</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62. 61%/86. 15%/34. 71% on DOTA/HRSC/FAIR1M. To our best knowledge, Point2R box is the world's first spatial layout detector.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">GBlobs Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain Li DAR-based 3D Object D 2503.08639v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>detectors</span>
        <span>current</span>
        <span>features</span>
        <span>point</span>
        <span>cloud</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose to exploit explicit local point cloud structure for DG, in particular by encoding point cloud neighborhoods with Gaussian blobs, GBlobs .</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Li DAR-based 3D detectors need large datasets for training, yet they struggle to generalize to novel domains. Domain Generalization (DG) aims to mitigate this by training detectors that are invariant to such domain shifts. Current DG approaches exclusively rely on global geometric features (point cloud Cartesian coordinates) as input features. Over-reliance on these global geometric features can, however, cause 3D detectors to prioritize object location and absolute position, resulting in poor cross-domain performance. To mitigate this, we propose to exploit explicit local point cloud structure for DG, in particular by encoding point cloud neighborhoods with Gaussian blobs, GBlobs . Our proposed formulation is highly efficient and requires no additional parameters. Without any bells and whistles, simply by integrating GBlobs in existing detectors, we beat the current state-of-the-art in challenging single-source DG benchmarks by over 21m AP (Waymo√ëKITTI), 13m AP (KITTI√ëWaymo), and 12m AP (nu Scenes √ëKITTI), without sacrificing in-domain performance. Additionally, GBlobs demonstrate exceptional performance in multi-source DG, surpassing the current state-of-the-art by and 5m AP on Waymo, KITTI, and ONCE, respectively.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Li DAR-based 3D detectors need large datasets for training, yet they struggle to generalize to novel domains. We propose to exploit explicit local point cloud structure for DG, in particular by encoding point cloud neighborhoods with Gaussian blobs. Our proposed formulation is highly efficient and requires no additional parameters. Without any bells and whistles, simply by integrating GBlobs in existing detectors, we beat the current state-of-the-art in challenging single-source DG benchmarks by over 21m AP.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Ra CFormer Towards High-Quality 3D Object Detection via Query-based Radar-Camera Fusion 2412.12725v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>radar-camera</span>
        <span>fusion</span>
        <span>former</span>
        <span>cformer</span>
        <span>object</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">We propose Radar-Camera fusion trans former (Ra CFormer) to boost the accuracy of 3D object detection by the following insight.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">We propose Radar-Camera fusion trans former (Ra CFormer) to boost the accuracy of 3D object detection by the following insight. The Radar-Camera fusion in outdoor 3D scene perception is capped by the image-to-BEV transformation‚Äìif the depth of pixels is not accurately estimated, the naive combination of BEV features actually integrates unaligned visual content. To avoid this problem, we propose a query-based framework that enables adaptively sample instance-relevant features from both the BEV and the original image view. Furthermore, we enhance system performance by two key designs: optimizing query initialization and strengthening the representational capacity of BEV . For the former, we introduce an adaptive circular distribution in polar coordinates to refine the initialization of object queries, allowing for a distance-based adjustment of query density. For the latter, we initially incorporate a radar-guided depth head to refine the transformation from image view to BEV . Subsequently, we focus on leveraging the Doppler effect of radar and introduce an implicit dynamic catcher to capture the temporal elements within the BEV . Extensive experiments on nu Scenes and View-of-Delft (Vo D) datasets validate the merits of our design. Remarkably, our method achieves superior results of 64.9% m AP and 70.2% NDS on nu Scenes, even outperforming several Li DAR-based detectors. Ra CFormer also secures the 1st ranking on the Vo D dataset. The code will be released.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">We propose Radar-Camera fusion trans former (Ra CFormer) to boost the accuracy of 3D object detection. The method achieves superior results of 64. 9% m AP and 70. 2% NDS on nu Scenes, even outperforming several Li DAR-based detectors. Ra CFormer secures the 1st ranking on the Vo D dataset. The code will be released in the near future. For confidential support, call the Samaritans on 08457 90, visit a local Samaritans branch or see www. samaritans. org.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Re RAW RAW-from-RGB Image Reconstruction via Stratified Sampling for Efficient Object Detection on th 2503.03782v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>models</span>
        <span>images</span>
        <span>datasets</span>
        <span>edge-based</span>
        <span>computer</span>
    </div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Edge-based computer vision models running on compact, resource-limited devices benefit greatly from using unprocessed, detail-rich RAW sensor data instead of processed RGB images. Training these models, however, necessitates large labeled RAW datasets, which are costly and often impractical to obtain. Thus, converting existing labeled RGB datasets into sensor-specific RAW images becomes crucial for effective model training.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Edge-based computer vision models running on compact, resource-limited devices benefit greatly from using unprocessed, detail-rich RAW sensor data instead of processed RGB images. Training these models necessitates large labeled RAW datasets, which are costly and often impractical to obtain. converting existing labeled RGB datasets into sensor-specific RAW images becomes crucial for effective model training. .</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">SEEN-DA SEmantic ENtropy guided Domain-aware Attention for Domain Adaptive Object Detection 2410.09004v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>knowledge</span>
        <span>domain</span>
        <span>adapter</span>
        <span>daod</span>
        <span>visual</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. As the visual-language models (VLMs) can provide essential general knowledge on unseen images, freezing the visual encoder and inserting a domain-agnostic adapter can learn domaininvariant knowledge for DAOD. However, the domain-agnostic adapter is inevitably biased to the source domain. It discards some beneficial knowledge discriminative on the unlabelled domain, i.e.domain-specific knowledge of the target domain. To solve the issue, we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is exploiting domain-specific knowledge between the essential general knowledge and domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter (DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter (DSA) for injecting the domain-specific knowledge from the information discarded by the visual encoder. Comprehensive experiments over multiple DAOD tasks show that DA-Ada can efficiently infer a domain-aware visual encoder for boosting domain adaptive object detection. Our code is available at [URL]</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. As the visual-language models (VLMs) can provide essential general knowledge on unseen images, freezing the visual encoder can learn domaininvariant knowledge for DAOD. However, the domain-agnostic adapter is inevitably biased to the source domain. To solve the issue, we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Cubify Anything Scaling Indoor 3D Object Detection 2412.04458v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>ca-1m</span>
        <span>scenes</span>
        <span>objects</span>
        <span>respect</span>
        <span>datasets</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce the Cubify-Anything 1M (CA-1M) dataset, which exhaustively labels over 400K 3D objects on over 1K highly accurate laser-scanned scenes with near-perfect registration to over 3.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">We consider indoor 3D object detection with respect to a single RGB(-D) frame acquired from a commodity handheld device. We seek to significantly advance the status quo with respect to both data and modeling. First, we establish that existing datasets have significant limitations to scale, accuracy, and diversity of objects. As a result, we introduce the Cubify-Anything 1M (CA-1M) dataset, which exhaustively labels over 400K 3D objects on over 1K highly accurate laser-scanned scenes with near-perfect registration to over 3.5K handheld, egocentric captures. Next, we establish Cubify Transformer (Cu TR), a fully Transformer 3D object detection baseline which rather than operating in 3D on point or voxel-based representations, predicts 3D ‚Ä†Project co-leadboxes directly from 2D features derived from RGB(-D) inputs. While this approach lacks any 3D inductive biases, we show that paired with CA-1M, Cu TR outperforms pointbased methods ‚Äî accurately recalling over 62% of objects in 3D, and is significantly more capable at handling noise and uncertainty present in commodity Li DAR-derived depth maps while also providing promising RGB only performance without architecture changes. Furthermore, by pre-training on CA-1M, Cu TR can outperform point-based methods on a more diverse variant of SUN RGB-D ‚Äî supporting the notion that while inductive biases in 3D are useful at the smaller sizes of existing datasets, they fail to scale to the data-rich regime of CA-1M. Overall, this dataset and baseline model provide strong evidence that we are moving towards models which can effectively Cubify Anything . 1ar Xiv: [cs.CV] 5 Dec 2024 SUN RGB-DScan Net v2ARKit Scenes ARKit Scenes Scan Net v2 SUN RGB-DCA-1M Figure 2. CA-1M is the first dataset to provide explicit 3D boxes which cover the full richness of objects while being both spatially accurate and pixel-perfect with respect to each frame. Existing datasets like SUN RGB-D, Scan Net vARKit Scenes are either small, coarsely labeled, or lack accurate mappings from world to image space. Since ARKit Scenes and CA-1M are labeled on the same underlying data, we can show the effect of exhaustive labeling.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Cubify-Anything 1M (CA-1M) dataset exhaustively labels over 400K 3D objects on over 1K highly accurate laser-scanned scenes. Cubify Transformer (Cu TR), a fully Transformer 3D object detection baseline, predicts 3D ‚Ä†Project co-leadboxes directly from 2D features derived from RGB(-D) inputs. Cu TR outperforms pointbased methods ‚Äî accurately recalling over 62% of objects in 3D. We hope this will be a useful tool in the future for building 3D models and 3D maps.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Towards RAW Object Detection in Diverse Conditions 2411.15678v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>detection</span>
        <span>pre-training</span>
        <span>object</span>
        <span>conditions</span>
        <span>domain</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">We introduce the AODRaw dataset, which offers 785 high-resolution real RAW images with 601 annotated instances spanning 62 categories, capturing a broad range of indoor and outdoor scenes under 9 distinct light and weather conditions.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Existing object detection methods often consider s RGB input, which was compressed from RAW data using ISP originally designed for visualization. However, such compression might lose crucial information for detection, especially under complex light and weather conditions. We introduce the AODRaw dataset, which offers 785 high-resolution real RAW images with 601 annotated instances spanning 62 categories, capturing a broad range of indoor and outdoor scenes under 9 distinct light and weather conditions. Based on AODRaw that supports RAW and s RGB object detection, we provide a comprehensive benchmark for evaluating current detection methods. We find that s RGB pre-training constrains the potential of RAW object detection due to the domain gap between s RGB and RAW, prompting us to directly pre-train on the RAW domain. However, it is harder for RAW pre-training to learn rich representations than s RGB pre-training due to the camera noise. To assist RAW pre-training, we distill the knowledge from an off-the-shelf model pre-trained on the s RGB domain. As a result, we achieve substantial improvements under diverse and adverse conditions without relying on extra pre-processing modules.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Existing object detection methods often consider s RGB input, which was compressed from RAW data using ISP originally designed for visualization. Such compression might lose crucial information for detection, especially under complex light and weather conditions. We find that s RGB pre-training constrains the potential of RAW object detection due to the domain gap between s RGB and RAW, prompting us to directly pre-train on the RAW domain. We introduce the AODRaw dataset, which offers 785 high-resolution real RAW images with 601 annotated instances spanning 62 categories, capturing a broad range of indoor and outdoor scenes.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Revisiting Generative Replay for Class Incremental Object Detection 2406.04829v4</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>objects</span>
        <span>methods</span>
        <span>incremental</span>
        <span>old-class</span>
        <span>object</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">Existing Incremental Object Detection (IOD) methods partially alleviate catastrophic forgetting when incrementally detecting new objects in real-world scenarios. However, many of these methods rely on the assumption that unlabeled oldclass objects may co-occur with labeled new-class objects in the incremental data. When unlabeled old-class objects are absent, the performance of existing methods tends to degrade. The absence can be mitigated by generating old-class samples, but it incurs high costs.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Existing Incremental Object Detection (IOD) methods partially alleviate catastrophic forgetting when incrementally detecting new objects in real-world scenarios. However, many of these methods rely on the assumption that unlabeled oldclass objects may co-occur with labeled new-class objects in the incremental data. When unlabeled old-class objects are absent, the performance of existing methods tends to degrade. The absence can be mitigated by generating old-class samples, but it incurs high costs.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Existing Incremental Object Detection (IOD) methods partially alleviate catastrophic forgetting when incrementally detecting new objects in real-world scenarios. Many of these methods rely on the assumption that unlabeled oldclass objects may co-occur with labeled new-class objects in the incremental data. The performance of existing methods tends to degrade. The absence can be mitigated by generating old-class samples, but it incurs high costs. In this paper, we.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Search and Detect Training-Free Long Tail Object Detection via Web-Image Retrieval 2409.18733v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>object</span>
        <span>detection</span>
        <span>search</span>
        <span>images</span>
        <span>improvement</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce Search Det, a trainingfree long-tail object detection framework that significantly enhances open-vocabulary object detection performance.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">In this paper, we introduce Search Det, a trainingfree long-tail object detection framework that significantly enhances open-vocabulary object detection performance. Search Det retrieves a set of positive and negative images of an object to ground, embeds these images, and computes an input image‚Äìweighted query which is used to detect the desired concept in the image. Our proposed method is simple and training-free, yet achieves over 48.7% m AP improvement on ODin W and 59.1% m AP improvement on LVIS compared to state-of-the-art models such as Grounding DINO. We further show that our approach of basing object detection on a set of Web-retrieved exemplars is stable with respect to variations in the exemplars, suggesting a path towards eliminating costly data annotation and training procedures.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Search Det retrieves a set of positive and negative images of an object to ground, embeds these images, and computes an input image‚Äìweighted query which is used to detect the desired concept in the image. As a solution, As a solution, As a solution, Our proposed method is simple and training-free, yet achieves over 48. 7% m AP improvement on ODin W and 59. 1% mAP improvement on LVIS compared to state-of-the-art models such as Grounding DINO. We further show that our approach of basing object detection on aSet of Web-retrieved exemplars is stable through respect to variations in the exemplars, suggesting a path towards eliminating costly data annotation and training procedures.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Uni Mamba Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for Li DAR-based  2503.12009v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>spatial</span>
        <span>mamba</span>
        <span>serialization</span>
        <span>global</span>
        <span>voxels</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a novel Unified Mamba (Uni Mamba), which seamlessly integrates the merits of 3D convolution and SSM in a concise multi-head manner, aiming to perform ‚Äúlocal and global‚Äù spatial context aggregation efficiently and simultaneously.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Recent advances in Li DAR 3D detection have demonstrated the effectiveness of Transformer-based frameworks in capturing the global dependencies from point cloud spaces, which serialize the 3D voxels into the flattened 1D sequence for iterative self-attention. However, the spatial structure of 3D voxels will be inevitably destroyed during the serialization process. Besides, due to the considerable number of 3D voxels and quadratic complexity of Transformers, multiple sequences are grouped before feeding to Transformers, leading to a limited receptive field. Inspired by the impressive performance of State Space Models (SSM), in this paper, we propose a novel Unified Mamba (Uni Mamba), which seamlessly integrates the merits of 3D convolution and SSM in a concise multi-head manner, aiming to perform ‚Äúlocal and global‚Äù spatial context aggregation efficiently and simultaneously. Specifically, a Uni Mamba block is designed which mainly consists of spatial locality modeling, complementary Z-order serialization and local-global sequential aggregator. The spatial locality modeling module integrates 3D submanifold convolution to capture the dynamic spatial position embedding before serialization. Then the efficient Z-order curve is adopted for serialization both horizontally and vertically. Furthermore, the local-global sequential aggregator adopts the channel grouping strategy to efficiently encode both ‚Äúlocal and global‚Äù spatial inter-dependencies using multi-head SSM. Additionally, an encoder-decoder architecture with stacked Uni Mamba blocks is formed to facilitate multi-scale spatial learning hierarchically. Extensive experiments are conducted on three popular datasets: nu Scenes, Waymo and Argoverse 2. Particularly, our Uni Mamba achieves 70.2 m AP on the nu Scenes dataset. ‚ô†</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">The Uni Mamba block is designed which mainly consists of spatial locality modeling, complementary Z-order serialization and local-global sequential aggregator. Extensive experiments are conducted on three popular datasets: nu Scenes, Waymo and Argoverse 2. Particularly, our Uni Mambas achieves 70. 2 m AP on the nu Scenes dataset. It is designed to perform ‚Äúlocal and global‚Äù spatial context aggregation efficiently and simultaneously. It has been used in the development of the Waymo self-driving car.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection 2503.09968v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>styles</span>
        <span>style</span>
        <span>method</span>
        <span>prompt</span>
        <span>information</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a new method, i.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Recently, a task of Single-Domain Generalized Object Detection (Single-DGOD) is proposed, aiming to generalize a detector to multiple unknown domains never seen before during training. Due to the unavailability of target-domain data, some methods leverage the multimodal capabilities of vision-language models, using textual prompts to estimate cross-domain information, enhancing the model‚Äôs generalization capability. These methods typically use a single textual prompt, often referred to as the one-step prompt method. However, when dealing with complex styles such as the combination of rain and night, we observe that the performance of the one-step prompt method tends to be relatively weak. The reason may be that many scenes incorporate not just a single style but a combination of multiple styles. The one-step prompt method may not effectively synthesize combined information involving various styles. To address this limitation, we propose a new method, i.e., Style Evolving along Chain-of-Thought, which aims to progressively integrate and expand style information along the chain of thought, enabling the continual evolution of styles. Specifically, by progressively refining style descriptions and guiding the diverse evolution of styles, this approach enables more accurate simulation of various style characteristics and helps the model gradually learn and adapt to subtle differences between styles. Additionally, it exposes the model to a broader range of style features with different data distributions, thereby enhancing its generalization capability in unseen domains. The significant performance gains over five adverse-weather scenarios and the Real to Art benchmark demonstrate the superiorities of our method.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection. The significant performance gains over five adverse-weather scenarios and the Real to Art benchmark demonstrate the superiorities of our method. By progressively refining style descriptions and guiding the diverse evolution of styles, this approach enables more accurate simulation of various style characteristics and helps the model gradually learn and adapt to subtle differences between styles. It exposes the model to a broader range of style features through different data distributions, thereby enhancing its generalization capability in unseen domains.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Mono TAKD Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection 2404.04910v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>model</span>
        <span>monocular</span>
        <span>performance</span>
        <span>teacher</span>
        <span>student</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we introduce Monocular Teaching Assistant Knowledge Distillation (Mono TAKD) to enhance 3D perception in Mono3D.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Monocular 3D object detection (Mono3D) holds noteworthy promise for autonomous driving applications owing to the cost-effectiveness and rich visual context of monocular camera sensors. However, depth ambiguity poses a significant challenge, as it requires extracting precise 3D scene geometry from a single image, resulting in suboptimal performance when transferring knowledge from a Li DARbased teacher model to a camera-based student model. To address this issue, we introduce Monocular Teaching Assistant Knowledge Distillation (Mono TAKD) to enhance 3D perception in Mono3D. Our approach presents a robust camera-based teaching assistant model that effectively bridges the representation gap between different modalities for teacher and student models, addressing the challenge of inaccurate depth estimation. By defining 3D spatial cues as residual features that capture the differences between the teacher and the teaching assistant models, we leverage these cues into the student model, improving its 3D perception capabilities. Experimental results show that our Mono TAKD achieves state-of-the-art performance on the KITTI3D dataset. Additionally, we evaluate the performance on nu Scenes and KITTI raw datasets to demonstrate the generalization of our model to multi-view 3D and unsupervised data settings. Our code will be available at [URL] TAKD .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Monocular 3D object detection (Mono3D) holds noteworthy promise for autonomous driving applications owing to the cost-effectiveness and rich visual context of monocular cameras. Depth ambiguity poses a significant challenge, as it requires extracting precise 3D scene geometry from a single image. To address this issue, we introduce Monocular Teaching Assistant Knowledge Distillation to enhance 3D perception in Mono3D. By defining 3D spatial cues as residual features that capture the differences between the teacher and the teaching assistant models, we leverage these cues into the student model.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Gau Cho Gaussian Distributions with Cholesky Decomposition for Oriented Object Detection 2502.01565v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>regression</span>
        <span>objects</span>
        <span>problem</span>
        <span>head</span>
        <span>loss</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a regression head for OOD that directly produces Gaussian distributions based on the Cholesky matrix decomposition.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Oriented Object Detection (OOD) has received increased attention in the past years, being a suitable solution for detecting elongated objects in remote sensing analysis. In particular, using regression loss functions based on Gaussian distributions has become attractive since they yield simple and differentiable terms. However, existing solutions are still based on regression heads that produce Oriented Bounding Boxes (OBBs), and the known problem of angular boundary discontinuity persists. In this work, we propose a regression head for OOD that directly produces Gaussian distributions based on the Cholesky matrix decomposition. The proposed head, named Gau Cho, theoretically mitigates the boundary discontinuity problem and is fully compatible with recent Gaussian-based regression loss functions. Furthermore, we advocate using Oriented Ellipses (OEs) to represent oriented objects, which relates to Gau Cho through a bijective function and alleviates the encoding ambiguity problem for circular objects. Our experimental results show that Gau Cho can be a viable alternative to the traditional OBB head, achieving results comparable to or better than state-of-the-art detectors for the challenging dataset DOTA. Our code will be available at [URL] .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">The proposed head, named Gau Cho, theoretically mitigates the boundary discontinuity problem and is fully compatible with recent Gaussian-based regression loss functions. We advocate using Oriented Ellipses (OEs) to represent oriented objects, which relates to Gau Cho through a bijective function and alleviates the encoding ambiguity problem for circular objects. Our experimental results show that Gau Cho can be a viable alternative to the traditional OBB head, achieving results comparable to or better than state-of-the-art detectors.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Object Detection using Event Camera A Mo E Heat Conduction based Detector and A New Benchmark Dataset 2412.06647v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>detection</span>
        <span>object</span>
        <span>event</span>
        <span>streams</span>
        <span>research</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">This paper introduces a novel Mo E (Mixture of Experts) heat conductionbased object detection algorithm that strikingly balances accuracy and computational efficiency.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Object detection in event streams has emerged as a cuttingedge research area, demonstrating superior performance in low-light conditions, scenarios with motion blur, and rapid movements. Current detectors leverage spiking neural networks, Transformers, or convolutional neural networks as their core architectures, each with its own set of limitations including restricted performance, high computational overhead, or limited local receptive fields. This paper introduces a novel Mo E (Mixture of Experts) heat conductionbased object detection algorithm that strikingly balances accuracy and computational efficiency. Initially, we employ a stem network for event data embedding, followed by processing through our innovative Mo E-HCO blocks. Each block integrates various expert modules to mimic heat conduction within event streams. Subsequently, an Io Ubased query selection module is utilized for efficient token extraction, which is then channeled into a detection head for the final object detection process. Furthermore, we are pleased to introduce Ev DET200K, a novel benchmark dataset for event-based object detection. Captured with a high-definition Prophesee EVK4-HD event camera, this dataset encompasses 10 distinct categories, 000 bounding boxes, and 054 samples, each spanning 2 to 5 seconds. We also provide comprehensive results from over 15 state-of-the-art detectors, offering a solid foundation for future research and comparison. The source code of this paper will be released on: [URL] AHU/Open Ev DET</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Object detection in event streams has emerged as a cuttingedge research area. Current detectors leverage spiking neural networks, Transformers, or convolutional neural networks as their core architectures. This paper introduces a novel Mo E (Mixture of Experts) heat conductionbased object detection algorithm that strikingly balances accuracy and computational efficiency. We also provide comprehensive results from over 15 state-of-the-art detectors, offering a solid foundation for future research and comparison. The source code of this paper will be released on: [URL] AHU/Open Ev DET.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Aero Gen Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation 2411.15497v3</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>data</span>
        <span>object</span>
        <span>rsiod</span>
        <span>detection</span>
        <span>specific</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose an end-to-end data augmentation framework that integrates a diversity-conditioned generator and a filter- *Corresponding author.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Remote sensing image object detection (RSIOD) aims to identify and locate specific objects within satellite or aerial imagery. However, there is a scarcity of labeled data in current RSIOD datasets, which significantly limits the performance of current detection algorithms. Although existing techniques, e.g., data augmentation and semisupervised learning, can mitigate this scarcity issue to some extent, they are heavily dependent on high-quality labeled data and perform worse in rare object classes. To address this issue, this paper proposes a layout-controllable diffusion generative model (i.e. Aero Gen) tailored for RSIOD. To our knowledge, Aero Gen is the first model to simultaneously support horizontal and rotated bounding box condition generation, thus enabling the generation of high-quality synthetic images that meet specific layout and object category requirements. Additionally, we propose an end-to-end data augmentation framework that integrates a diversity-conditioned generator and a filter- Corresponding author.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Aero Gen Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation. Remote sensing image object detection (RSIOD) aims to identify and locate specific objects within satellite or aerial imagery. To our knowledge, Aero Gen is the first model to simultaneously support horizontal and rotated bounding box condition generation, thus enabling the generation of high-quality synthetic images. We propose an end-to-end data augmentation framework that integrates a diversity-conditioned generator and a filter.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">SP3D Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts 2503.06467v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>semantic</span>
        <span>performance</span>
        <span>accurate</span>
        <span>sp3d</span>
        <span>prompts</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose a boosting strategy, termed SP3D, explicitly utilizing the cross-modal semantic prompts generated from Large Multimodal Models (LMMs) to boost the 3D detector with robust feature discrimination capability under sparse annotation settings.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Recently, sparsely-supervised 3D object detection has gained great attention, achieving performance close to fully-supervised 3D detectors while requiring only a few annotated instances. Nevertheless, these methods suffer challenges when accurate labels are extremely absent. In this paper, we propose a boosting strategy, termed SP3D, explicitly utilizing the cross-modal semantic prompts generated from Large Multimodal Models (LMMs) to boost the 3D detector with robust feature discrimination capability under sparse annotation settings. Specifically, we first develop a Confident Points Semantic Transfer (CPST ) module that generates accurate cross-modal semantic prompts through boundary-constrained center cluster selection. Based on these accurate semantic prompts, which we treat as seed points, we introduce a Dynamic Cluster Pseudo-label Generation ( DCPG ) module to yield pseudo-supervision signals from the geometry shape of multi-scale neighbor points. Additionally, we design a Distribution Shape score ( DS score ) that chooses high quality supervision signals for the initial training of the 3D detector. Experiments on the KITTI dataset and Waymo Open Dataset (WOD) have validated that SP3D can enhance the performance of sparsely supervised detectors by a large margin under meager labeling conditions. Moreover, we verified SP3D in the zero-shot setting, where its performance exceeded that of the state-of-the-art methods. The code is available at [URL] xmuqimingxia/SP3D .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">SP3D can enhance the performance of sparsely supervised detectors by a large margin under meager labeling conditions. We first develop a Confident Points Semantic Transfer (CPST ) module that generates accurate cross-modal semantic prompts through boundary-constrained center cluster selection. Based on these accurate semantic prompts, which we treat as seed points, we introduce a Dynamic Cluster Pseudo-label Generation ( DCPG ) module to yield pseudo-supervision signals. We design a Distribution Shape score ( DS score ) that chooses high quality supervision signals for the initial training of the 3D detector.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Efficient Event-Based Object Detection A Hybrid Neural Network with Spatial and Temporal Attention 2403.10173v3</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>networks</span>
        <span>neuromorphic</span>
        <span>hardware</span>
        <span>hybrid</span>
        <span>snn-ann</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we present a variant that integrates DWConv LSTMs to the ANN blocks to capture slower dynamics.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. While Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy efficient and low latency event-based data processing, they often fall short of Artificial Neural Networks (ANNs) in accuracy and flexibility. Here, we introduce Attention-based Hybrid SNN-ANN backbones for event-based object detection to leverage the strengths of both SNN and ANN architectures. A novel Attention-based SNN-ANN bridge module captures sparse spatial and temporal relations from the SNN layer and converts them into dense feature maps for the ANN part of the backbone. Additionally, we present a variant that integrates DWConv LSTMs to the ANN blocks to capture slower dynamics. This multi-timescale network combines fast SNN processing for short timesteps with long-term dense RNN processing, effectively capturing both fast and slow dynamics. Experimental results demonstrate that our proposed method surpasses SNN-based approaches by significant margins, with results comparable to existing ANN and RNN-based methods. Unlike ANN-only networks, the hybrid setup allows us to implement the SNN blocks on digital neuromorphic hardware to investigate the feasibility of our approach. Extensive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. Our hybrid SNN-ANN architectures pave the way for ANN-like performance at a drastically reduced parameter, latency, and power budget.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy efficient and low latency event-based data processing. Here, we introduce Attention-based Hybrid SNN-ANN backbones to leverage the strengths of both SNN and ANN architectures. This multi-timescale network combines fast SNN processing for short timesteps with long-term dense RNN processing, effectively capturing both fast and slow dynamics.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Fractal Calibration for long-tailed object detection 2410.11774v2</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>fracal</span>
        <span>class</span>
        <span>classes</span>
        <span>method</span>
        <span>datasets</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">we propose FRA ctal CAL ibration (FRACAL): a novel post-calibration method for long-tailed object detection.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Real-world datasets follow an imbalanced distribution, which poses significant challenges in rare-category object detection. Recent studies tackle this problem by developing re-weighting and re-sampling methods, that utilise the class frequencies of the dataset. However, these techniques focus solely on the frequency statistics and ignore the distribution of the classes in image space, missing important information. In contrast to them, we propose FRA ctal CAL ibration (FRACAL): a novel post-calibration method for long-tailed object detection. FRACAL devises a logit adjustment method that utilises the fractal dimension to estimate how uniformly classes are distributed in image space. During inference, it uses the fractal dimension to inversely downweight the probabilities of uniformly spaced class predictions achieving balance in two axes: between frequent and rare categories, and between uniformly spaced and sparsely spaced classes. FRACAL is a post-processing method and it does not require any training, also it can be combined with many off-the-shelf models such as one-stage sigmoid detectors and two-stage instance segmentation models. FRACAL boosts the rare class performance by up to 8.6%and surpasses all previous methods on LVIS dataset, while showing good generalisation to other datasets such as COCO, V3Det and Open Images. We provide the code at https: //github.com/kostas1515/FRACAL .</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Fractal Calibration (FRACAL) is a novel post-calibration method for long-tailed object detection. It uses the fractal dimension to inversely downweight the probabilities of uniformly spaced class predictions. FRACAL boosts the rare class performance by up to 8. 6% and surpasses all previous methods on LVIS dataset. It can be combined with many off-the-shelf models such as one-stage sigmoid detectors and two-stage instance segmentation models. It is a post-processing method and it does not require any training.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Test-Time Backdoor Detection for Object Detection Models 2503.15293v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>detection</span>
        <span>object</span>
        <span>samples</span>
        <span>test</span>
        <span>backdoor</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">Object detection models are vulnerable to backdoor attacks, where attackers poison a small subset of training samples by embedding a predefined trigger to manipulate prediction. Detecting poisoned samples (i.e., those containing triggers) at test time can prevent backdoor activation. However, unlike image classification tasks, the unique characteristics of object detection‚Äîparticularly its output of numerous objects‚Äîpose fresh challenges for backdoor detection. The complex attack effects (e.g., ‚Äúghost‚Äù object emergence or ‚Äúvanishing‚Äù object) further render current defenses fundamentally inadequate. To this end, we design TRA nsformation Consistency Evaluation ( TRACE ), a brand-new method for detecting poisoned samples at test time in object detection. Our journey begins with two intriguing observations: 1)poisoned samples exhibit significantly more consistent detection results than clean ones across varied backgrounds. 2)clean samples show higher detection consistency when introduced to different focal information. Based on these phenomena, TRACE applies foreground and background transformations to each test sample, then assesses transformation consistency by calculating the variance in objects confidences. TRACE achieves black-box, universal backdoor detection, with extensive experiments showing a 30% improvement in AUROC over state-of-the-art defenses and resistance to adaptive attacks.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Object detection models are vulnerable to backdoor attacks, where attackers poison a small subset of training samples by embedding a predefined trigger to manipulate prediction. Detecting poisoned samples (i.e., those containing triggers) at test time can prevent backdoor activation. However, unlike image classification tasks, the unique characteristics of object detection‚Äîparticularly its output of numerous objects‚Äîpose fresh challenges for backdoor detection. The complex attack effects (e.g., ‚Äúghost‚Äù object emergence or ‚Äúvanishing‚Äù object) further render current defenses fundamentally inadequate. To this end, we design TRA nsformation Consistency Evaluation ( TRACE ), a brand-new method for detecting poisoned samples at test time in object detection. Our journey begins with two intriguing observations: 1)poisoned samples exhibit significantly more consistent detection results than clean ones across varied backgrounds. 2)clean samples show higher detection consistency when introduced to different focal information. Based on these phenomena, TRACE applies foreground and background transformations to each test sample, then assesses transformation consistency by calculating the variance in objects confidences. TRACE achieves black-box, universal backdoor detection, with extensive experiments showing a 30% improvement in AUROC over state-of-the-art defenses and resistance to adaptive attacks.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Object detection models are vulnerable to backdoor attacks, where attackers poison a small subset of training samples by embedding a predefined trigger. Detecting poisoned samples (i. e. , those containing triggers) at test time can prevent backdoor activation. Our journey begins with two intriguing observations: 1)poisoned samples exhibit significantly more consistent detection results than clean ones across varied backgrounds. 2)clean samples show higher detection consistency when introduced to different focal information. Based on these phenomena, TRACE applies foreground and background transformations to each test sample, then assesses transformation consistency by calculating the variance in objects confidences.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-container">
    <div class="paper-header">
        <h2 class="paper-title">Sparse Align a Fully Sparse Framework for Cooperative Object Detection 2503.12982v1</h2>
        <span class="layout-badge">Figures before Abstract</span>
    </div>
    <div class="section-title"><h3>Keywords</h3></div>
    <div class="keywords">
        <span>sparse</span>
        <span>detection</span>
        <span>cooperative</span>
        <span>perception</span>
        <span>view</span>
    </div>
    <div class="section-title method-section-title"><h3>Proposed Method</h3></div>
    <div class="method-description">Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird‚Äôs Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, Sparse Align, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and Dair V2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and Dair V2Xt datasets for timealigned cooperative object detection also show a significant performance gain compared to the baseline works.</div>
    <div class="section-title"><h3>Original Abstract</h3></div>
    <div class="abstract">Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird‚Äôs Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, Sparse Align, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and Dair V2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and Dair V2Xt datasets for timealigned cooperative object detection also show a significant performance gain compared to the baseline works.</div>
    <div class="section-title"><h3>Summary</h3></div>
    <div class="summary">Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird‚Äôs Eye View (BEV) feature maps. In this work, we design a fully sparse framework, Sparse Align, with three key features: an enhanced sparse 3D backbone, a query-based temporal context module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and Dair V2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements.</div>
    <div class="paper-metadata">
        <details>
            <summary>Paper Layout Information</summary>
            <ul>
                <li>Has figures before abstract: Yes</li>
            </ul>
        </details>
    </div>
</div>
<div class="paper-metadata">
    <h3>Summary Statistics</h3>
    <ul>
        <li>Total papers processed: 27</li>
        <li>Papers with abstracts: 27</li>
        <li>Papers with summaries: 27</li>
        <li>Papers with figures before abstract: 27</li>
    </ul>
    <details>
        <summary>Papers with Non-Standard Layouts</summary>
        <ul>
            <li>V2X-R Cooperative Li DAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection 2411.08402v3</li>
            <li>UCOD-DPL Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning 2407.13157v1</li>
            <li>Samba A Unified Mamba-based Framework for General Salient Object Detection 2305.05260v1</li>
            <li>Sim LTD Simple Semi-Supervised Long-Tailed Object Detection 2412.20047v1</li>
            <li>Ev-3DOD Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras 2502.19630v1</li>
            <li>MI-DETR An Object Detection Model with Multi-time Inquiries Mechanism 2503.01463v1</li>
            <li>Re Diff Det Rotation-equivariant Diffusion Model for Oriented Object Detection 2103.07733v1</li>
            <li>Point2RBox-v2 Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instan 2502.04268v2</li>
            <li>GBlobs Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain Li DAR-based 3D Object D 2503.08639v1</li>
            <li>Ra CFormer Towards High-Quality 3D Object Detection via Query-based Radar-Camera Fusion 2412.12725v1</li>
            <li>Re RAW RAW-from-RGB Image Reconstruction via Stratified Sampling for Efficient Object Detection on th 2503.03782v1</li>
            <li>SEEN-DA SEmantic ENtropy guided Domain-aware Attention for Domain Adaptive Object Detection 2410.09004v1</li>
            <li>Cubify Anything Scaling Indoor 3D Object Detection 2412.04458v1</li>
            <li>Towards RAW Object Detection in Diverse Conditions 2411.15678v1</li>
            <li>Revisiting Generative Replay for Class Incremental Object Detection 2406.04829v4</li>
            <li>Search and Detect Training-Free Long Tail Object Detection via Web-Image Retrieval 2409.18733v1</li>
            <li>Uni Mamba Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for Li DAR-based  2503.12009v2</li>
            <li>Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection 2503.09968v1</li>
            <li>Mono TAKD Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection 2404.04910v2</li>
            <li>Gau Cho Gaussian Distributions with Cholesky Decomposition for Oriented Object Detection 2502.01565v1</li>
            <li>Object Detection using Event Camera A Mo E Heat Conduction based Detector and A New Benchmark Dataset 2412.06647v1</li>
            <li>Aero Gen Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation 2411.15497v3</li>
            <li>SP3D Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts 2503.06467v1</li>
            <li>Efficient Event-Based Object Detection A Hybrid Neural Network with Spatial and Temporal Attention 2403.10173v3</li>
            <li>Fractal Calibration for long-tailed object detection 2410.11774v2</li>
            <li>Test-Time Backdoor Detection for Object Detection Models 2503.15293v1</li>
            <li>Sparse Align a Fully Sparse Framework for Cooperative Object Detection 2503.12982v1</li>
        </ul>
    </details>
</div>
</body>
</html>